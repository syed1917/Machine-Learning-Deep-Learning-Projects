{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: Syed Muzammil Syed Riyaz Ahamed\n",
    "- ID: 9012161\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\syedm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\syedm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\syedm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\syedm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\syedm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\syedm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt'), nltk.download('punkt_tab'), nltk.download('stopwords'), nltk.download('wordnet'), nltk.download('averaged_perceptron_tagger_eng'), nltk.download('maxent_ne_chunker_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['python',\n",
       "  'is',\n",
       "  'a',\n",
       "  'versatile',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'python',\n",
       "  'proved',\n",
       "  'it',\n",
       "  'importance',\n",
       "  'in',\n",
       "  'various',\n",
       "  'domain'],\n",
       " ['javascript', 'is', 'widely', 'used', 'for', 'web', 'development'],\n",
       " ['java', 'is', 'known', 'for', 'it', 'platform', 'independence'],\n",
       " ['programming', 'involves', 'writing', 'code', 'to', 'solve', 'problem'],\n",
       " ['data', 'structure', 'are', 'crucial', 'for', 'efficient', 'programming'],\n",
       " ['algorithm', 'are', 'instruction', 'for', 'solving', 'problem'],\n",
       " ['version',\n",
       "  'control',\n",
       "  'system',\n",
       "  'help',\n",
       "  'manage',\n",
       "  'code',\n",
       "  'change',\n",
       "  'in',\n",
       "  'collaboration'],\n",
       " ['debugging',\n",
       "  'is',\n",
       "  'the',\n",
       "  'process',\n",
       "  'of',\n",
       "  'finding',\n",
       "  'and',\n",
       "  'fixing',\n",
       "  'error',\n",
       "  'in',\n",
       "  'python',\n",
       "  'code'],\n",
       " ['web',\n",
       "  'framework',\n",
       "  'simplify',\n",
       "  'the',\n",
       "  'development',\n",
       "  'of',\n",
       "  'web',\n",
       "  'application'],\n",
       " ['artificial',\n",
       "  'intelligence',\n",
       "  'can',\n",
       "  'be',\n",
       "  'applied',\n",
       "  'in',\n",
       "  'various',\n",
       "  'programming',\n",
       "  'task']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_sentences(sentences=None):\n",
    "\n",
    "    if sentences is None:\n",
    "        return []\n",
    "\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize sentence into words\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Perform POS tagging for NER\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Perform Named Entity Recognition (NER)\n",
    "        ner_chunks = ne_chunk(pos_tags)\n",
    "        \n",
    "        processed_tokens = []\n",
    "        for chunk in ner_chunks:\n",
    "            if isinstance(chunk, Tree):  # This is a named entity\n",
    "                entity = \" \".join(c[0] for c in chunk)  # Join tokens of the named entity\n",
    "                # Only add alphanumeric entities\n",
    "                if re.match(r'^[a-zA-Z0-9\\s]+$', entity):\n",
    "                    processed_tokens.append(entity.lower())  # Normalize by case-folding\n",
    "            else:\n",
    "                word, tag = chunk\n",
    "                # Remove special characters and normalize word\n",
    "                if re.match(r'^[a-zA-Z0-9]+$', word):\n",
    "                    normalized_word = lemmatizer.lemmatize(word.lower())\n",
    "                    processed_tokens.append(normalized_word)\n",
    "        \n",
    "        # Append the processed tokens as a sentence\n",
    "        tokenized_sentences.append(processed_tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "cleaned_data = clean_sentences(sample_sentences)\n",
    "cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in': [1, 7, 8, 10], 'python': [1, 8], 'proved': [1], 'domain': [1], 'importance': [1], 'various': [1, 10], 'language': [1], 'versatile': [1], 'programming': [1, 4, 5, 10], 'it': [1, 3], 'a': [1], 'is': [1, 2, 3, 8], 'widely': [2], 'used': [2], 'development': [2, 9], 'javascript': [2], 'for': [2, 3, 5, 6], 'web': [2, 9], 'platform': [3], 'known': [3], 'independence': [3], 'java': [3], 'problem': [4, 6], 'writing': [4], 'to': [4], 'involves': [4], 'code': [4, 7, 8], 'solve': [4], 'structure': [5], 'are': [5, 6], 'data': [5], 'efficient': [5], 'crucial': [5], 'instruction': [6], 'algorithm': [6], 'solving': [6], 'help': [7], 'version': [7], 'manage': [7], 'control': [7], 'collaboration': [7], 'change': [7], 'system': [7], 'and': [8], 'fixing': [8], 'finding': [8], 'process': [8], 'error': [8], 'the': [8, 9], 'debugging': [8], 'of': [8, 9], 'application': [9], 'framework': [9], 'simplify': [9], 'artificial': [10], 'applied': [10], 'be': [10], 'can': [10], 'intelligence': [10], 'task': [10]}\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "\n",
    "    # Initialize an empty dictionary to store the inverted index\n",
    "    inverted_index = {}\n",
    "    \n",
    "    # Iterate through sentences with their 1-indexed sentence ID\n",
    "    for sentence_id, sentence in enumerate(list_of_sentence_tokens, 1):\n",
    "        # Iterate through unique tokens in the sentence\n",
    "        for token in set(sentence):\n",
    "            # If token not in index, create a new list\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            \n",
    "            # Add sentence ID to the token's list if not already present\n",
    "            if sentence_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(sentence_id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "# Print the result\n",
    "result = get_inverted_index(cleaned_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': {1: [0, 6], 8: [10]}, 'is': {1: [1], 2: [1], 3: [1], 8: [1]}, 'a': {1: [2]}, 'versatile': {1: [3]}, 'programming': {1: [4], 4: [0], 5: [6], 10: [7]}, 'language': {1: [5]}, 'proved': {1: [7]}, 'it': {1: [8], 3: [4]}, 'importance': {1: [9]}, 'in': {1: [10], 7: [7], 8: [9], 10: [5]}, 'various': {1: [11], 10: [6]}, 'domain': {1: [12]}, 'javascript': {2: [0]}, 'widely': {2: [2]}, 'used': {2: [3]}, 'for': {2: [4], 3: [3], 5: [4], 6: [3]}, 'web': {2: [5], 9: [0, 6]}, 'development': {2: [6], 9: [4]}, 'java': {3: [0]}, 'known': {3: [2]}, 'platform': {3: [5]}, 'independence': {3: [6]}, 'involves': {4: [1]}, 'writing': {4: [2]}, 'code': {4: [3], 7: [5], 8: [11]}, 'to': {4: [4]}, 'solve': {4: [5]}, 'problem': {4: [6], 6: [5]}, 'data': {5: [0]}, 'structure': {5: [1]}, 'are': {5: [2], 6: [1]}, 'crucial': {5: [3]}, 'efficient': {5: [5]}, 'algorithm': {6: [0]}, 'instruction': {6: [2]}, 'solving': {6: [4]}, 'version': {7: [0]}, 'control': {7: [1]}, 'system': {7: [2]}, 'help': {7: [3]}, 'manage': {7: [4]}, 'change': {7: [6]}, 'collaboration': {7: [8]}, 'debugging': {8: [0]}, 'the': {8: [2], 9: [3]}, 'process': {8: [3]}, 'of': {8: [4], 9: [5]}, 'finding': {8: [5]}, 'and': {8: [6]}, 'fixing': {8: [7]}, 'error': {8: [8]}, 'framework': {9: [1]}, 'simplify': {9: [2]}, 'application': {9: [7]}, 'artificial': {10: [0]}, 'intelligence': {10: [1]}, 'can': {10: [2]}, 'be': {10: [3]}, 'applied': {10: [4]}, 'task': {10: [8]}}\n"
     ]
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "\n",
    "    # Initialize an empty dictionary to store the positional index\n",
    "    positional_index = {}\n",
    "    \n",
    "    # Iterate through sentences with their IDs (starting from 1)\n",
    "    for sentence_id, sentence in enumerate(list_of_sentence_tokens, 1):\n",
    "        # Iterate through tokens in the sentence\n",
    "        for position, token in enumerate(sentence):\n",
    "            # If token not in index, create a new dictionary\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = {}\n",
    "            \n",
    "            # If sentence ID not in token's dictionary, create a new list\n",
    "            if sentence_id not in positional_index[token]:\n",
    "                positional_index[token][sentence_id] = []\n",
    "            \n",
    "            # Add the position to the token's list of positions for this sentence\n",
    "            positional_index[token][sentence_id].append(position)\n",
    "    \n",
    "    return positional_index\n",
    "\n",
    "# Assuming cleaned_data is the output from clean_sentences()\n",
    "positional_index = get_positional_index(cleaned_data)\n",
    "print(positional_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146, 0.123, 0.0, 0.0, 0.0, 0.0, 0.123, 0.138, 0.0, 0.0, 0.0, 0.146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.123, 0.146, 0.277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.138, 0.146, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.257, 0.271, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.229, 0.257, 0.271, 0.0, 0.271, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.257, 0.0, 0.229, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271], [0.0, 0.0, 0.0, 0.0, 0.0, 0.257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271, 0.271, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.317, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.189, 0.211, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0], [0.0, 0.0, 0.158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.142, 0.0, 0.0, 0.0, 0.0, 0.158, 0.0, 0.0, 0.0, 0.158, 0.158, 0.158, 0.0, 0.0, 0.0, 0.0, 0.133, 0.0, 0.0, 0.0, 0.0, 0.133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.158, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.211, 0.211, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "\n",
    "    # Step 1: Get unique tokens across all documents\n",
    "    unique_tokens = sorted(set(token for sentence in list_of_sentence_tokens for token in sentence))\n",
    "    \n",
    "    # Step 2: Calculate Document Frequency (DF)\n",
    "    doc_frequency = {}\n",
    "    for token in unique_tokens:\n",
    "        doc_frequency[token] = sum(1 for sentence in list_of_sentence_tokens if token in sentence)\n",
    "    \n",
    "    # Step 3: Calculate Term Frequency (TF) and IDF\n",
    "    total_docs = len(list_of_sentence_tokens)\n",
    "    tf_idf_matrix = []\n",
    "    \n",
    "    for sentence_id, sentence in enumerate(list_of_sentence_tokens, 1):\n",
    "        # Calculate term frequencies for this document\n",
    "        term_freq = {}\n",
    "        for token in sentence:\n",
    "            term_freq[token] = term_freq.get(token, 0) + 1\n",
    "        \n",
    "        # Normalize term frequency\n",
    "        doc_length = len(sentence)\n",
    "        normalized_tf = {token: count/doc_length for token, count in term_freq.items()}\n",
    "        \n",
    "        # Calculate TF-IDF for each token\n",
    "        doc_tf_idf = []\n",
    "        for token in unique_tokens:\n",
    "            # Calculate IDF\n",
    "            idf = total_docs / (doc_frequency[token] + 1)\n",
    "            idf = max(0, 1 + (1 - (doc_frequency[token] / total_docs))) if idf > 0 else 0\n",
    "            \n",
    "            # Calculate TF-IDF\n",
    "            tf_idf_value = normalized_tf.get(token, 0) * idf\n",
    "            doc_tf_idf.append(round(tf_idf_value, 3))\n",
    "        \n",
    "        tf_idf_matrix.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "# Assuming cleaned_data is the output from clean_sentences()\n",
    "tf_idf_matrix = get_TFIDF_matrix(cleaned_data)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 7, 9, 1, 2, 3, 4, 6] \n",
      "\n",
      "[6, 7, 3, 9, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    # Tokenize the search query (assuming same preprocessing as sentences)\n",
    "    \n",
    "    query_tokens = search_query.lower().split()\n",
    "    \n",
    "    # Handle different ranking methods\n",
    "    if method_name == \"inverted\":\n",
    "        # Inverted Index Ranking: Count matching tokens\n",
    "        document_scores = []\n",
    "        for doc_id, sentence in enumerate(list_of_sentence_tokens):\n",
    "            # Convert sentence to lowercase for case-insensitive matching\n",
    "            lower_sentence = [token.lower() for token in sentence]\n",
    "            \n",
    "            # Count matching query tokens\n",
    "            match_count = sum(1 for query_token in query_tokens if query_token in lower_sentence)\n",
    "            \n",
    "            document_scores.append((doc_id, match_count))\n",
    "        \n",
    "        # Sort by match count in descending order\n",
    "        ranked_docs = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return only the document indices\n",
    "        return [doc[0] for doc in ranked_docs if doc[1] > 0]\n",
    "    \n",
    "    elif method_name == \"tfidf\":\n",
    "        # TF-IDF Ranking: Calculate TF-IDF scores for query\n",
    "        # First, calculate the TF-IDF matrix\n",
    "        def calculate_tfidf():\n",
    "            # Get unique tokens\n",
    "            unique_tokens = sorted(set(token for sentence in list_of_sentence_tokens for token in sentence))\n",
    "            \n",
    "            # Calculate Document Frequency (DF)\n",
    "            doc_frequency = {}\n",
    "            for token in unique_tokens:\n",
    "                doc_frequency[token] = sum(1 for sentence in list_of_sentence_tokens if token in sentence)\n",
    "            \n",
    "            # Total number of documents\n",
    "            total_docs = len(list_of_sentence_tokens)\n",
    "            \n",
    "            # TF-IDF matrix\n",
    "            tf_idf_matrix = []\n",
    "            \n",
    "            for sentence in list_of_sentence_tokens:\n",
    "                # Calculate term frequencies\n",
    "                term_freq = {}\n",
    "                for token in sentence:\n",
    "                    term_freq[token] = term_freq.get(token, 0) + 1\n",
    "                \n",
    "                # Normalize term frequency\n",
    "                doc_length = len(sentence)\n",
    "                normalized_tf = {token: count/doc_length for token, count in term_freq.items()}\n",
    "                \n",
    "                # Calculate document vector\n",
    "                doc_vector = []\n",
    "                for token in unique_tokens:\n",
    "                    # Calculate IDF\n",
    "                    idf = total_docs / (doc_frequency[token] + 1)\n",
    "                    idf = max(0, 1 + (1 - (doc_frequency[token] / total_docs))) if idf > 0 else 0\n",
    "                    \n",
    "                    # Calculate TF-IDF\n",
    "                    tf_idf_value = normalized_tf.get(token, 0) * idf\n",
    "                    doc_vector.append(max(0, tf_idf_value))\n",
    "                \n",
    "                tf_idf_matrix.append(doc_vector)\n",
    "            \n",
    "            return tf_idf_matrix, unique_tokens\n",
    "        \n",
    "        # Calculate TF-IDF matrix and unique tokens\n",
    "        tf_idf_matrix, unique_tokens = calculate_tfidf()\n",
    "        \n",
    "        # Calculate query vector\n",
    "        query_vector = []\n",
    "        for token in unique_tokens:\n",
    "            # Calculate query term frequency\n",
    "            query_tf = query_tokens.count(token) / len(query_tokens)\n",
    "            \n",
    "            # Calculate IDF for query token\n",
    "            query_df = sum(1 for sentence in list_of_sentence_tokens if token in sentence)\n",
    "            query_idf = len(list_of_sentence_tokens) / (query_df + 1)\n",
    "            query_idf = max(0, 1 + (1 - (query_df / len(list_of_sentence_tokens)))) if query_idf > 0 else 0\n",
    "            \n",
    "            # Calculate TF-IDF for query token\n",
    "            query_vector.append(query_tf * query_idf)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        def cosine_similarity(vec1, vec2):\n",
    "            # Dot product\n",
    "            dot_product = sum(a*b for a, b in zip(vec1, vec2))\n",
    "            \n",
    "            # Magnitude of vectors\n",
    "            magnitude1 = (sum(a*a for a in vec1)) ** 0.5\n",
    "            magnitude2 = (sum(b*b for b in vec2)) ** 0.5\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if magnitude1 * magnitude2 == 0:\n",
    "                return 0\n",
    "            \n",
    "            return dot_product / (magnitude1 * magnitude2)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        document_scores = []\n",
    "        for doc_id, doc_vector in enumerate(tf_idf_matrix):\n",
    "            similarity = cosine_similarity(query_vector, doc_vector)\n",
    "            document_scores.append((doc_id, similarity))\n",
    "        \n",
    "        # Sort by similarity score in descending order\n",
    "        ranked_docs = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return only the document indices\n",
    "        return [doc[0] for doc in ranked_docs if doc[1] > 0]\n",
    "    \n",
    "    else:\n",
    "        # Invalid method\n",
    "        raise ValueError(\"Invalid method. Choose 'inverted' or 'tfidf'.\")\n",
    "    \n",
    "# Assuming cleaned_data is your list of tokenized sentences\n",
    "# Inverted Index Method\n",
    "inverted_ranks = get_ranked_documents(cleaned_data, \"inverted\", sample_sentences[0])\n",
    "\n",
    "# TF-IDF Method\n",
    "tfidf_ranks = get_ranked_documents(cleaned_data, \"tfidf\", sample_sentences[6])\n",
    "\n",
    "print(f\"{inverted_ranks} \\n\")\n",
    "print(tfidf_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.466, 0.188, 0.0, 0.0, 0.0, 0.0, 1.083, 0.392, 0.0, 0.0, 0.0, 0.776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.449, 0.582, 6.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.286, 1.164, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.083, 0.0, 0.0, 5.278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.319, 0.0, 0.0, 0.0, 0.636, 1.759, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.633, 0.0, 0.0, 0.0, 0.0, 0.754, 0.0, 0.0, 0.0, 1.083, 0.706, 5.278, 0.0, 1.759, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.639, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.566, 0.0, 2.246, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 1.056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.759], [0.0, 0.0, 0.0, 0.0, 0.0, 1.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.319, 5.278, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.321, 0.0, 0.0, 0.0, 0.0, 0.0, 2.639, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 5.433, 0.0, 0.0, 0.0, 1.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.633, 0.0, 0.0, 0.0, 0.0, 0.0, 1.811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.66, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.718, 0.464, 0.558, 2.513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.256, 0.0, 0.258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.025, 0.0, 0.0, 0.0], [0.0, 0.0, 0.677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.232, 0.0, 0.0, 0.0, 0.0, 4.737, 0.0, 0.0, 0.0, 0.526, 0.789, 0.592, 0.0, 0.0, 0.0, 0.0, 0.207, 0.0, 0.0, 0.0, 0.0, 1.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.706, 0.0, 0.0, 1.184, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.643, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.882, 0.0, 0.0, 0.0, 0.0, 0.0, 7.635, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.005, 0.0, 5.025, 1.256, 1.675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.345, 0.0, 0.0, 2.513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.558, 0.0, 0.0, 0.0, 0.49, 0.0, 0.0, 0.0, 0.0, 0.0]] \n",
      "\n",
      "[[1.784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.535, 0.251, 0.0, 0.0, 0.0, 0.0, 1.43, 0.469, 0.0, 0.0, 0.0, 0.892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.588, 0.669, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.344, 1.338, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.43, 0.0, 0.0, 5.974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.494, 0.0, 0.0, 0.0, 0.752, 1.991, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.807, 0.0, 0.0, 0.0, 0.0, 0.853, 0.0, 0.0, 0.0, 1.43, 0.845, 5.974, 0.0, 1.991, 0.0, 0.0, 0.0, 0.996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.665, 0.0, 2.94, 0.0, 0.0, 0.0, 0.996, 0.0, 0.0, 0.0, 0.0, 0.0, 1.195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.991], [0.0, 0.0, 0.0, 0.0, 0.0, 1.552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.494, 5.974, 0.0, 0.0, 0.0, 0.996, 0.0, 0.0, 0.0, 0.646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.0, 2.987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 6.129, 0.0, 0.0, 0.0, 2.328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.807, 0.0, 0.0, 0.0, 0.0, 0.0, 2.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.817, 0.58, 0.636, 2.861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.43, 0.0, 0.345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.722, 0.0, 0.0, 0.0], [0.0, 0.0, 0.776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 5.433, 0.0, 0.0, 0.0, 0.604, 0.905, 0.679, 0.0, 0.0, 0.0, 0.0, 0.276, 0.0, 0.0, 0.0, 0.0, 1.43, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.845, 0.0, 0.0, 1.358, 0.0, 0.0, 0.364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.947, 0.0, 0.0, 0.0, 0.0, 0.0, 1.056, 0.0, 0.0, 0.0, 0.0, 0.0, 9.025, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.144, 0.0, 5.722, 1.43, 1.907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46, 0.0, 0.0, 2.861, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.636, 0.0, 0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "def get_modified_tfidf_matrix(list_of_sentence_tokens, M=5):\n",
    "\n",
    "    ## TODO: Implement the functionality that will return the modified tf-idf matrix\n",
    "   \n",
    "    # Step 1: Extract unique tokens and sort them\n",
    "    unique_tokens = sorted(set(token for sentence in list_of_sentence_tokens for token in sentence))\n",
    "    \n",
    "    # Step 2: Calculate document frequencies and document lengths\n",
    "    doc_frequency = {}\n",
    "    doc_lengths = []\n",
    "    \n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        # Calculate document length\n",
    "        doc_lengths.append(len(sentence))\n",
    "        \n",
    "        # Calculate document frequencies\n",
    "        unique_sent_tokens = set(sentence)\n",
    "        for token in unique_sent_tokens:\n",
    "            doc_frequency[token] = doc_frequency.get(token, 0) + 1\n",
    "    \n",
    "    \n",
    "    # Total number of documents\n",
    "    total_docs = len(list_of_sentence_tokens)\n",
    "    \n",
    "    # Step 3: Calculate Modified TF-IDF matrix\n",
    "    modified_tf_idf_matrix = []\n",
    "    \n",
    "    for doc_id, sentence in enumerate(list_of_sentence_tokens):\n",
    "        # Calculate Modified Term Frequency (MTF) for each token\n",
    "        mtf_vector = []\n",
    "        \n",
    "        for token in unique_tokens:\n",
    "            # Count of token in document\n",
    "            token_count = sentence.count(token)\n",
    "            \n",
    "            # Find first position of token\n",
    "            first_position = sentence.index(token) if token in sentence else len(sentence)\n",
    "            \n",
    "            # Modified Term Frequency (MTF)\n",
    "            mtf = token_count / (1 + first_position) if token_count > 0 else 0\n",
    "            \n",
    "            # Modified Inverse Document Frequency (MIDF)\n",
    "            # Calculate sum of lengths of documents containing the token\n",
    "            docs_with_token = sum(\n",
    "                len(doc) for doc in list_of_sentence_tokens \n",
    "                if token in doc\n",
    "            )\n",
    "            \n",
    "            # Modified IDF calculation\n",
    "            midf = 0\n",
    "            if doc_frequency.get(token, 0) > 0:\n",
    "                midf = max(0, \n",
    "                    # log of (total docs / (doc freq * normalized avg length of docs with token))\n",
    "                    1 + log(\n",
    "                        total_docs / \n",
    "                        (doc_frequency[token] * (docs_with_token / (total_docs * M)))\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            # Final Modified TF-IDF weight\n",
    "            modified_tf_idf = mtf * midf\n",
    "            \n",
    "            mtf_vector.append(round(modified_tf_idf, 3))\n",
    "        \n",
    "        modified_tf_idf_matrix.append(mtf_vector)\n",
    "    \n",
    "    return modified_tf_idf_matrix\n",
    "\n",
    "# Helper function for logarithm to avoid importing math library\n",
    "def log(x):\n",
    "\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    \n",
    "    # Simple logarithm approximation\n",
    "    n = 1000.0\n",
    "    return n * ((x ** (1/n)) - 1)\n",
    "\n",
    "# Default M value\n",
    "modified_matrix = get_modified_tfidf_matrix(cleaned_data)\n",
    "\n",
    "# Experimenting with different M values\n",
    "matrix_low_m = get_modified_tfidf_matrix(cleaned_data, M=5)   # More length sensitivity\n",
    "matrix_high_m = get_modified_tfidf_matrix(cleaned_data, M=10)  # Less length sensitivity\n",
    "\n",
    "print(f\"{matrix_low_m} \\n\")\n",
    "print(matrix_high_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Experimental Analysis of Scaling Factor M:**\n",
    "##### **Role of M in Modified TF-IDF:**\n",
    "##### Scaling factor M is an important parameter in our modified TF-IDF method, introducing a subtler method of normalization for document length's influence on term importance. The major roles played by the scaling factor M are as follows:\n",
    "\n",
    "##### **Document Length Normalization**\n",
    "- ##### A method that offers a balancing effect to the weight of terms across documents of different lengths.\n",
    "- ##### Prevents longer documents from dominating the computation of term importance.\n",
    "\n",
    "##### **Flexibility in Term Weighting**\n",
    "- ##### Allows fine-tuning of how document length influences term relevance.\n",
    "- ##### Provides a knob to adjust the sensitivity of the weighting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 1:\n",
      "  Max Weight: 4.4090\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 3.4378\n",
      "  Avg Min Weight: 0.0000\n",
      "\n",
      "M = 2:\n",
      "  Max Weight: 5.7980\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 4.2368\n",
      "  Avg Min Weight: 0.0000\n",
      "\n",
      "M = 5:\n",
      "  Max Weight: 7.6350\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 5.2938\n",
      "  Avg Min Weight: 0.0000\n",
      "\n",
      "M = 10:\n",
      "  Max Weight: 9.0250\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 6.0940\n",
      "  Avg Min Weight: 0.0000\n",
      "\n",
      "M = 20:\n",
      "  Max Weight: 10.4170\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 6.9247\n",
      "  Avg Min Weight: 0.0000\n",
      "\n",
      "M = 50:\n",
      "  Max Weight: 12.2580\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 8.0302\n",
      "  Avg Min Weight: 0.0000\n",
      "\n",
      "M = 100:\n",
      "  Max Weight: 13.6520\n",
      "  Min Weight: 0.0000\n",
      "  Avg Max Weight: 8.8670\n",
      "  Avg Min Weight: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def experiment_m_values(cleaned_data):\n",
    "    # M values to experiment with\n",
    "    m_values = [1, 2, 5, 10, 20, 50, 100]\n",
    "    \n",
    "    # Store results for analysis\n",
    "    results = []\n",
    "    \n",
    "    for m in m_values:\n",
    "        # Calculate modified TF-IDF matrix\n",
    "        modified_matrix = get_modified_tfidf_matrix(cleaned_data, M=m)\n",
    "        \n",
    "        # Calculate some statistics\n",
    "        max_values = [max(row) for row in modified_matrix]\n",
    "        min_values = [min(row) for row in modified_matrix]\n",
    "        \n",
    "        results.append({\n",
    "            'M': m,\n",
    "            'max_weight': max(max_values),\n",
    "            'min_weight': min(min_values),\n",
    "            'avg_max_weight': sum(max_values) / len(max_values),\n",
    "            'avg_min_weight': sum(min_values) / len(min_values)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Assuming cleaned_data is available\n",
    "m_analysis = experiment_m_values(cleaned_data)\n",
    "for result in m_analysis:\n",
    "    print(f\"M = {result['M']}:\")\n",
    "    print(f\"  Max Weight: {result['max_weight']:.4f}\")\n",
    "    print(f\"  Min Weight: {result['min_weight']:.4f}\")\n",
    "    print(f\"  Avg Max Weight: {result['avg_max_weight']:.4f}\")\n",
    "    print(f\"  Avg Min Weight: {result['avg_min_weight']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Observations and Interpretation**\n",
    "1. ##### **Low M Values (M = 1-2)**\n",
    "    - ##### Characteristics:\n",
    "        - ##### Extreme sensitivity to document length.\n",
    "        - ##### Heavy penalty for longer documents.\n",
    "        - ##### Narrow range of term weights.\n",
    "\n",
    "    - ##### Impact:\n",
    "        - ##### Terms in shorter documents receive disproportionately high weights.\n",
    "        - ##### Probably excessively highlighting rare terms in short documents.\n",
    "\n",
    "2. ##### Moderate M Values (M = 5-10)\n",
    "    - ##### Characteristics:\n",
    "        - ##### Balanced approach for document length normalization.\n",
    "        - ##### Reasonable spread of term weights.\n",
    "        - ##### Most representative regarding characteristics of real-world text.\n",
    "\n",
    "    - ##### Impact:\n",
    "        - ##### Represents the middle ground between sensitivity for length and importance of terms.\n",
    "        - ##### Suitable for most general text analysis tasks.\n",
    "\n",
    "3. ##### High M Values: M = 50-100\n",
    "    - ##### Characteristics:\n",
    "        - ##### Very minimal document length factor effect on term weighting.\n",
    "        - ##### Larger spread of term weights.\n",
    "        - ##### More like traditional TF-IDF schemes.\n",
    "\n",
    "    - ##### Impact:\n",
    "        - ##### Treats the document length as almost negligible.\n",
    "        - ##### Gives more importance to the frequency of the term and document frequency.\n",
    "\n",
    "##### **Theoretical Interpretation**\n",
    "##### The scaling factor M in MIDF formula adjusts the contribution of document length to term weightings: \n",
    "##### $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "##### for higher M the weight of the term is lower; thus, it reduces the role of document length.\n",
    "\n",
    "##### **Suggested Approach**\n",
    "- ##### **Start with M = 5**\n",
    "    - ##### Mostly balanced\n",
    "    - ##### For almost every text analysis application\n",
    "\n",
    "- ##### **Apply Based on Application**\n",
    "    - ##### Document type: academic papers, tweets, articles\n",
    "    - ##### Corpus-specific characteristics\n",
    "    - ##### Specific goals regarding the retrieval of information\n",
    "\n",
    "##### **Practical Implications**\n",
    "- ##### **Natural Language Processing**\n",
    "    - ##### Document classification\n",
    "    - ##### Information retrieval\n",
    "    - ##### Sentiment analysis\n",
    "\n",
    "- ##### Research Application\n",
    "    - ##### Academic text analysis\n",
    "    - ##### Comparative document studies\n",
    "\n",
    "##### **Limitations**\n",
    "- ##### Domain-dependent validation is required.\n",
    "- ##### Weight calculation involves computational overhead.\n",
    "\n",
    "##### **Conclusion**\n",
    "##### The scaling factor M offers a flexible method for normalizing term importance by accounting for document length. Tuning M to your corpus and goals is key to optimizing its effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Critical Analysis of Modified TF-Modified IDF**\n",
    "##### **Strengths**\n",
    "\n",
    "1. ##### **Contextual Sensitivity**\n",
    "\n",
    "    - ##### Uses position-weighting, so terms early in a document are given greater weight.\n",
    "    - ##### Follows human reading patterns where introductory terms have more weight in context.\n",
    "\n",
    "2. ##### **Document Length Normalization**\n",
    "\n",
    "    - ##### Reduces bias towards longer documents.\n",
    "    - ##### Employs a variable scaling parameter (M) to achieve refined weighting of terms.\n",
    "\n",
    "3. ##### **Advanced Weighting System**\n",
    "\n",
    "    - ##### Combines term frequency with position, document length, and context for multi-dimensional weighting.\n",
    "\n",
    "##### **Constraints**\n",
    "\n",
    "1. ##### **Computational Complexity**\n",
    "\n",
    "    - ##### More computationally expensive than standard TF-IDF.\n",
    "    - ##### Might not scale well for large corpora or real-time applications. \n",
    "\n",
    "2. ##### **Parameter Sensitivity**\n",
    "\n",
    "    - ##### Performance strongly relies on the tuning of the scaling factor M.\n",
    "    - ##### There is no universally optimal M.\n",
    "\n",
    "3. ##### **Potential Over-Engineering**\n",
    "\n",
    "    - ##### May add more complexity to simpler data sets or applications.\n",
    "\n",
    "##### **Contrast View**\n",
    "- ##### Vs Standard TF-IDF: \n",
    "    - ##### It affords richer, more nuanced term representation. \n",
    "    - ##### Addresses limitations such as document-length bias.\n",
    "- ##### Versus Advanced Techniques: \n",
    "    - ##### More interpretable and lightweight than neural approaches. \n",
    "    - ##### Balances simplicity with sophistication. \n",
    "##### Recommended Use Cases \n",
    "- ##### Academic Texts: Scientific papers, research articles. \n",
    "- ##### Information Retrieval: Search engines, classification systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'performance_score': {'mean_weight': 0.9200240963855425, 'max_weight': 4.409, 'min_weight': 0.042, 'weight_variance': 1.083540336768762, 'weight_skewness': 1.8756299935019634, 'unique_weight_ratio': 0.7469879518072289, 'weight_concentration': 0.04819277108433735}, 'interpretability_score': {'sparsity_ratio': 0.8616666666666667, 'weight_entropy': 3.919240760436458, 'term_consistency': -1.7560585055567761}}, 5: {'performance_score': {'mean_weight': 1.4875060240963858, 'max_weight': 7.635, 'min_weight': 0.188, 'weight_variance': 2.4871102981564803, 'weight_skewness': 2.0929674841005146, 'unique_weight_ratio': 0.7590361445783133, 'weight_concentration': 0.04819277108433735}, 'interpretability_score': {'sparsity_ratio': 0.8616666666666667, 'weight_entropy': 3.9964932227804684, 'term_consistency': -1.756108777301461}}, 10: {'performance_score': {'mean_weight': 1.73221686746988, 'max_weight': 9.025, 'min_weight': 0.251, 'weight_variance': 3.304314844534766, 'weight_skewness': 2.1584093310583445, 'unique_weight_ratio': 0.7349397590361446, 'weight_concentration': 0.07228915662650602}, 'interpretability_score': {'sparsity_ratio': 0.8616666666666667, 'weight_entropy': 4.008083370523387, 'term_consistency': -1.7561141212903921}}, 50: {'performance_score': {'mean_weight': 2.3010602409638556, 'max_weight': 12.258, 'min_weight': 0.398, 'weight_variance': 5.698772225286688, 'weight_skewness': 2.2699623904569677, 'unique_weight_ratio': 0.7590361445783133, 'weight_concentration': 0.04819277108433735}, 'interpretability_score': {'sparsity_ratio': 0.8616666666666667, 'weight_entropy': 4.02201934563511, 'term_consistency': -1.7560927996584905}}}\n"
     ]
    }
   ],
   "source": [
    "# Pseudo-code highlighting key considerations\n",
    "def calculate_performance(modified_matrix):\n",
    "\n",
    "    # Flatten the matrix for overall analysis\n",
    "    all_weights = [weight for row in modified_matrix for weight in row if weight > 0]\n",
    "    \n",
    "    # Performance metrics\n",
    "    performance_metrics = {\n",
    "        # Statistical measures\n",
    "        'mean_weight': sum(all_weights) / len(all_weights) if all_weights else 0,\n",
    "        'max_weight': max(all_weights) if all_weights else 0,\n",
    "        'min_weight': min(all_weights) if all_weights else 0,\n",
    "        \n",
    "        # Dispersion measures\n",
    "        'weight_variance': calculate_variance(all_weights),\n",
    "        'weight_skewness': calculate_skewness(all_weights),\n",
    "        \n",
    "        # Concentration metrics\n",
    "        'unique_weight_ratio': len(set(all_weights)) / len(all_weights),\n",
    "        'weight_concentration': calculate_concentration(all_weights)\n",
    "    }\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "def assess_interpretability(modified_matrix):\n",
    "\n",
    "    # Analyze term weight distribution across documents\n",
    "    interpretability_metrics = {\n",
    "        # Sparsity - how many zero weights\n",
    "        'sparsity_ratio': calculate_sparsity(modified_matrix),\n",
    "        \n",
    "        # Entropy of weight distribution\n",
    "        'weight_entropy': calculate_entropy(modified_matrix),\n",
    "        \n",
    "        # Consistency of term importance across documents\n",
    "        'term_consistency': calculate_term_consistency(modified_matrix)\n",
    "    }\n",
    "    \n",
    "    return interpretability_metrics\n",
    "\n",
    "# Helper functions for calculations\n",
    "def calculate_variance(values):\n",
    "\n",
    "    if not values:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate mean\n",
    "    mean = sum(values) / len(values)\n",
    "    \n",
    "    # Calculate variance\n",
    "    variance = sum((x - mean) ** 2 for x in values) / len(values)\n",
    "    return variance\n",
    "\n",
    "def calculate_skewness(values):\n",
    "\n",
    "    if len(values) < 3:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean = sum(values) / len(values)\n",
    "    std_dev = (sum((x - mean) ** 2 for x in values) / len(values)) ** 0.5\n",
    "    \n",
    "    # Calculate skewness\n",
    "    if std_dev == 0:\n",
    "        return 0\n",
    "    \n",
    "    skewness = sum(((x - mean) / std_dev) ** 3 for x in values) / len(values)\n",
    "    return skewness\n",
    "\n",
    "def calculate_concentration(values):\n",
    "\n",
    "    if not values:\n",
    "        return 0\n",
    "    \n",
    "    # Count unique values and their frequencies\n",
    "    unique_values = {}\n",
    "    for value in values:\n",
    "        unique_values[value] = unique_values.get(value, 0) + 1\n",
    "    \n",
    "    # Calculate concentration as max frequency ratio\n",
    "    max_frequency = max(unique_values.values())\n",
    "    return max_frequency / len(values)\n",
    "\n",
    "def calculate_sparsity(matrix):\n",
    "\n",
    "    total_elements = sum(len(row) for row in matrix)\n",
    "    zero_elements = sum(sum(1 for weight in row if weight == 0) for row in matrix)\n",
    "    \n",
    "    return zero_elements / total_elements if total_elements > 0 else 0\n",
    "\n",
    "def calculate_entropy(matrix):\n",
    "\n",
    "    # Flatten matrix and filter non-zero weights\n",
    "    weights = [weight for row in matrix for weight in row if weight > 0]\n",
    "    \n",
    "    if not weights:\n",
    "        return 0\n",
    "    \n",
    "    # Normalize weights\n",
    "    total = sum(weights)\n",
    "    probabilities = [w / total for w in weights]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -sum(p * log(p) for p in probabilities if p > 0)\n",
    "    return entropy\n",
    "\n",
    "def calculate_term_consistency(matrix):\n",
    "\n",
    "    if not matrix or not matrix[0]:\n",
    "        return 0\n",
    "    \n",
    "    # Transpose matrix to get term-wise weights\n",
    "    term_weights = list(zip(*matrix))\n",
    "    \n",
    "    # Calculate variation coefficient for each term\n",
    "    consistency_scores = []\n",
    "    for term_column in term_weights:\n",
    "        # Skip if all weights are zero\n",
    "        if all(w == 0 for w in term_column):\n",
    "            continue\n",
    "        \n",
    "        mean = sum(term_column) / len(term_column)\n",
    "        std_dev = (sum((x - mean) ** 2 for x in term_column) / len(term_column)) ** 0.5\n",
    "        \n",
    "        # Coefficient of variation (normalized standard deviation)\n",
    "        # Lower value means more consistent\n",
    "        consistency_score = std_dev / mean if mean != 0 else 0\n",
    "        consistency_scores.append(1 - consistency_score)  # Invert so higher is better\n",
    "    \n",
    "    # Average consistency across terms\n",
    "    return sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0\n",
    "\n",
    "# Custom log function (as before)\n",
    "def log(x):\n",
    "\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    \n",
    "    # Simple logarithm approximation\n",
    "    n = 1000.0\n",
    "    return n * ((x ** (1/n)) - 1)\n",
    "\n",
    "# Full evaluation function\n",
    "def evaluate_modified_tfidf(corpus):\n",
    "\n",
    "    # Experiment with different M values\n",
    "    m_values = [1, 5, 10, 50]\n",
    "    results = {}\n",
    "    \n",
    "    for m in m_values:\n",
    "        # Calculate modified matrix\n",
    "        modified_matrix = get_modified_tfidf_matrix(corpus, M=m)\n",
    "        \n",
    "        # Evaluate performance metrics\n",
    "        results[m] = {\n",
    "            'performance_score': calculate_performance(modified_matrix),\n",
    "            'interpretability_score': assess_interpretability(modified_matrix)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_modified_tfidf(corpus):\n",
    "    \n",
    "    # Experiment with different M values\n",
    "    m_values = [1, 5, 10, 50]\n",
    "    results = {}\n",
    "    \n",
    "    for m in m_values:\n",
    "        # Calculate modified matrix\n",
    "        modified_matrix = get_modified_tfidf_matrix(corpus, M=m)\n",
    "        \n",
    "        # Evaluate performance metrics\n",
    "        results[m] = {\n",
    "            'performance_score': calculate_performance(modified_matrix),\n",
    "            'interpretability_score': assess_interpretability(modified_matrix)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(evaluate_modified_tfidf(cleaned_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Theoretical and Practical Insights**\n",
    "##### **Theoretical Contribution**\n",
    "- ##### Challenges uniform term importance through position and length.\n",
    "- ##### Offers a dynamic, context-sensitive approach to term weighting.\n",
    "\n",
    "##### **Practical Limitations**\n",
    "- ##### Needs validation in domain and thus is not universal.\n",
    "- ##### Corpus characteristics can be a varying factor.\n",
    "\n",
    "##### **Conclusion: Is Modified TF-Modified IDF Good?**\n",
    "##### **Qualified Recommendation:** Yes, but with caveats.\n",
    "\n",
    "##### **When to Use:**\n",
    "- ##### Well-structured formal text - for example, academic, legal\n",
    "- ##### Applications that place a strong value on document structure\n",
    "- ##### Applications where added complexity can be supported computationally\n",
    "\n",
    "##### When to Avoid:\n",
    "- ##### Very large unstructured corpora\n",
    "- ##### Real-time or low latency applications\n",
    "- ##### Domains where textual structures are irregular.\n",
    "\n",
    "##### Key Takeaways\n",
    "- ##### A sophisticated tool-not a universal solution.\n",
    "- ##### Widens the horizon to understand text representation.\n",
    "- ##### Precious when applied judiciously in context-sensitive situations.\n",
    "##### The power of the technique lies in challenging the traditional assumptions and offering deeper insights into text analysis, emphasizing thoughtful, case-specific use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
